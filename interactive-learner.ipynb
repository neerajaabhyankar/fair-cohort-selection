{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author : Neeraja Abhyankar\n",
    "#Date : May 2018\n",
    "\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import math\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticks\n",
    "\n",
    "#sys.path.insert(0, os.getcwd()+'/static/pycode/')\n",
    "import funcs\n",
    "import GreedySubmodKnapsackWeighted\n",
    "import DSoptCardinalityConstrained\n",
    "import GreedyCardinalityConstrained\n",
    "import GreedyOnVrouge\n",
    "\n",
    "#import thresholdsL\n",
    "\n",
    "def create_results(n_dim, algo, randrunnum, L):\n",
    "    \"\"\"\n",
    "    This script runs the online learning scenario where\n",
    "    the uncertainty sampling objective is optimized using various algorithms.\n",
    "    \n",
    "    Parameter descriptions:\n",
    "        N_samples: number of rounds in the interactive experiment\n",
    "        feature_based_func_type: the type of the concave function you want to use\n",
    "            1: x.^gamma with 0 < gamma < 1;\n",
    "            2: log (1+gamma*x) with gamma >0;\n",
    "            3: 1 - gamma^(-x) with gamma > 1;\n",
    "            4: 1/(1+exp^(-gamma * x)) - 0.5;\n",
    "        gamma: the parameter defining the concave function\n",
    "        N_dim: number of modular features used\n",
    "        noise_level: the parameter associated with the linear regression problem\n",
    "        N_bins: feedback quantization\n",
    "        eta: additive noise to the feedback in SNR to simulate quantization\n",
    "    \"\"\"\n",
    "    \n",
    "    # SOME VARIABLES -- FIXED\n",
    "    \n",
    "    N_ground = 100 # this number should be fixed in this case\n",
    "    V = np.arange(1, N_ground+1)\n",
    "    N_dim = 628 # this number should also be fixed\n",
    "    K_budget = 10 # this number should be fixed\n",
    "    \n",
    "    feature_based_func_type_feedback = 2 # this is the function giving us feedback (in place of v-rouge)\n",
    "    gamma_feedback = 100\n",
    "    feature_based_func_type = 2 # this is the function we're modeling it as\n",
    "    gamma = 100\n",
    "    noise_level = 0.1\n",
    "    threshold_zero_flag = 1\n",
    "    #lambda_regul = float(1)/pow(noise_level,2)\n",
    "    lambda_regul = 1000\n",
    "    #noise_offset = 0.0\n",
    "    #less_items_flag = 1 #20choose5 instead of 100choose10\n",
    "    #if less_items_flag == 1:\n",
    "        #N_ground = \n",
    "        #K_budget = \n",
    "    \n",
    "    # RANDOM VARIABLES\n",
    "    #algo = 'random' ###############################\n",
    "    alpha_ind = 0 ##################################\n",
    "    alphachoice_str = ['0', '0_1', '1', '10', 'inf', 'lin', 'auto']\n",
    "    #idx = 0 #######################################\n",
    "    eta = 100 #SNR in dB/bit\n",
    "    \n",
    "    # THRESHOLD LEARNING PARAMETERS\n",
    "    #L = 2 #number of BITS\n",
    "    #thresh_learn_init = thresholdsL.thresh[L] ##2\n",
    "    \n",
    "    # FIXED FOR NOW, potentially changeable\n",
    "    N_samples = 1000\n",
    "    idxrange = 14\n",
    "    \n",
    "    #data_dir = ''\n",
    "    data_dir = 'vrouge_FINAL/'\n",
    "    \n",
    "    #NUMBER OF FEATURES\n",
    "    if n_dim != 0:\n",
    "        N_dim = n_dim\n",
    "    \n",
    "    outfile = alphachoice_str[alpha_ind]\n",
    "    outfile = 'Algo_' + algo + randrunnum + '_Nsmps_' + str(N_samples) + '_L_' + str(L) + '_alpha_' + alphachoice_str[alpha_ind] + '_collections_14' + '_human_' + 'vrouge_small' + str(N_dim)\n",
    "    print('creating', outfile)\n",
    "    \n",
    "    while os.path.exists(data_dir + outfile + '.save') == True:\n",
    "        outfile = outfile + '_repeated' #ensures unique names \n",
    "    \n",
    "    savedict = {}\n",
    "    savedict['Performance'] = np.array([0.0 for i in range(N_samples)])\n",
    "    savedict['Learned_Vec'] = {}\n",
    "    savedict['Thresh'] = {}\n",
    "    savedict['Queried_Summaries'] = {}\n",
    "    savedict['Queried_Scores'] = {}\n",
    "    savedict['Unquant_Scores'] = {}\n",
    "    \n",
    "    for idx in range(idxrange):   \n",
    "    \n",
    "        # FEATURE MATRIX\n",
    "        All_Data_Mat = {} #empty dictionary\n",
    "        io.loadmat('data-reduceddim-facloc-full.mat', mdict=All_Data_Mat);\n",
    "        Data_Mat_Complete = All_Data_Mat[str(idx)]\n",
    "        Data_Mat = Data_Mat_Complete[:N_dim, :]\n",
    "        \n",
    "        alphachoice = [0, 0.1, 1, 10, None]\n",
    "        if alpha_ind < 5:\n",
    "            alpha = alphachoice[alpha_ind]\n",
    "            \n",
    "        if alpha_ind == 6:\n",
    "            alpha = 0.001 ###############to start with\n",
    "    \n",
    "        # LOADING PROCESSED DATASET INFO\n",
    "        \n",
    "        proc_data = {} #empty dictionary\n",
    "        io.loadmat('processed_data.mat', mdict=proc_data);\n",
    "        \n",
    "        subset = proc_data['all_subset'][0][idx]\n",
    "        \n",
    "        #preproc_rand_summ_sc = {} #empty dictionary\n",
    "        #io.loadmat('preprocessed_random_summaries_and_scores.mat', mdict=preproc_rand_summ_sc);\n",
    "        \n",
    "        \"\"\"\n",
    "        A whole bunch of variables have been added from the preprocessed data\n",
    "        \n",
    "        proc_data.keys()\n",
    "        ['K_budget', '__header__', 'idx', '__globals__', 'imagecollection', 'all_random_scores', 'all_human_score', 'V_rouge_optimized_summary', 'K', 'N_dim', 'all_Feature_Vec', 'all_subset', 'Best_score', 'V', 'Data_Mat', 'N_images', 'V_rouge_optimized_summary_score', '__version__', 'Best_SummarySet', 'test_image_collection']\n",
    "        \n",
    "        preproc_rand_summ_sc.keys()\n",
    "        ['__header__', 'Collected_All_Random_Rouge_Scores', 'Collected_All_Random_Summaries', '__globals__', '__version__']\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # PRIORS\n",
    "        #thresh_learn = thresh_learn_init[idx]\n",
    "        mu_0 = np.array([1.0 for i in range(N_dim)]) # initialize the mean\n",
    "        mu_0 = mu_0/np.sum(mu_0)\n",
    "        sigma_0 = np.identity(N_dim) # initialize the covariance matrix\n",
    "        # ESTIMATES TO UPDATE\n",
    "        Cinv = lambda_regul * np.linalg.inv(sigma_0)\n",
    "        Yvec = np.transpose(np.atleast_2d(np.dot(Cinv, mu_0))) ## N_dim x 1\n",
    "        # LEARNT\n",
    "        w_vec = np.squeeze(np.dot(np.linalg.inv(Cinv), Yvec)) ## N_dim\n",
    "        mu_0 = None\n",
    "        sigma_0 = None\n",
    "            \n",
    "        # STUFF TO STORE\n",
    "        perf_vec = np.array([0.0 for i in range(N_samples)]) ## 100\n",
    "        thresh_learn_vec = np.array([0.0 for i in range(N_samples)]) ## 100\n",
    "        coeff_vec = np.array([[0.0,]*(np.size(Data_Mat, axis = 0)) for i in range(N_samples)]) ## 100 x 628\n",
    "        unquantized_score_vec = np.array([0.0 for i in range(N_samples)]) ## 100\n",
    "        Uncertainty_Queried_Summaries = np.array([[0.0,]*K_budget for i in range(N_samples)], dtype=int) ## 100 x 10\n",
    "        UQSummaries_dict = {} #for searching purposes : keys=sets, values=indexes\n",
    "        Uncertainty_Queried_Scores = np.array([0.0 for i in range(N_samples)]) ## 100\n",
    "        \n",
    "        # USED FOR NORMALIZING\n",
    "        # run greedy on Vrouge itself -- use access to the chosen summaries \"subset\"\n",
    "        max_opt_summary = GreedyOnVrouge.GreedyOnVrouge(subset, np.transpose(Data_Mat_Complete), V, K_budget, [1 for i in range(N_ground)], 1)\n",
    "        # best possible y\n",
    "        ymax = funcs.score_image_summarization(subset, np.transpose(Data_Mat_Complete), max_opt_summary)[0]\n",
    "        # worst possible y\n",
    "        ymin = 1000\n",
    "        for randmin in range(200):\n",
    "            SummarySet = np.random.choice(range(N_ground), K_budget, replace=False)\n",
    "            ymintemp = funcs.score_image_summarization(subset, np.transpose(Data_Mat_Complete), SummarySet)[0]\n",
    "            ymin = min(ymin, ymintemp)\n",
    "\n",
    "\n",
    "        for jdx in range(N_samples):\n",
    "            print('jdx = ', jdx)\n",
    "            \n",
    "            if alpha_ind == 5:\n",
    "                # VARY LINEARLY\n",
    "                if jdx < N_samples/3:\n",
    "                    alpha = 0.1\n",
    "                elif jdx < 2*N_samples/3:\n",
    "                    alpha = 1\n",
    "                else:\n",
    "                    alpha = 10\n",
    "                    \n",
    "            # FIND QUERY\n",
    "            #IF REPEATED, RECORD OLD SCORE AND PROCEED\n",
    "            #IF TOO HARD, JUST SELECT A RANDOM SUMMARY\n",
    "            \n",
    "            uniquesummflag = 0 #haven't found a unique summary yet\n",
    "            uniqueiters = 0 #tried this many times\n",
    "            while uniquesummflag == 0:\n",
    "                \n",
    "                #find a summary\n",
    "                #w_mediate = np.array([0.0 for i in range(N_dim)])\n",
    "                w_mediate = w_vec\n",
    "                 \n",
    "                if algo == 'random':\n",
    "                    SummarySet = np.random.choice(range(N_ground), K_budget, replace=False)\n",
    "                    \n",
    "                if algo == 'dsopt':\n",
    "                    SummarySet = DSoptCardinalityConstrained.DSoptCardinalityConstrained(np.transpose(Data_Mat), feature_based_func_type, w_mediate, np.linalg.inv(Cinv), alpha, V, K_budget, gamma)\n",
    "            \n",
    "                if algo == 'greedy':\n",
    "                    SummarySet = GreedyCardinalityConstrained.GreedyCardinalityConstrained(np.transpose(Data_Mat), feature_based_func_type, w_mediate, np.linalg.inv(Cinv), alpha, V, K_budget, gamma)\n",
    "                    \n",
    "                #see if it's unique\n",
    "                #dict has been used to compare\n",
    "                uniquesummflag = 1 #temporarily we think this is unique\n",
    "                if frozenset(SummarySet) in UQSummaries_dict.keys():\n",
    "                    print(\"repeated\")\n",
    "                    uniqueiters += 1\n",
    "                    jjdx = UQSummaries_dict[frozenset(SummarySet)]\n",
    "                    uniquesummflag = 0\n",
    "\n",
    "                    #copy score from jjdx and update Cinv, Yvec, w_vec\n",
    "                    #do not store summary in log\n",
    "                \n",
    "                    #training data\n",
    "                    x = funcs.featurize_data(Data_Mat, feature_based_func_type, gamma, SummarySet)\n",
    "                    x = np.atleast_2d(x) ## 1 x N_dim\n",
    "                    y = Uncertainty_Queried_Scores[jjdx]\n",
    "                    #learn\n",
    "                    Cinv += float(1)/pow(noise_level,2) * np.dot(np.transpose(x), x)\n",
    "                    Yvec += np.transpose(float(1)/pow(noise_level,2) * np.dot(y, x))\n",
    "                    w_vec = np.squeeze(np.dot(np.linalg.inv(Cinv), Yvec))\n",
    "                    if threshold_zero_flag == 1:\n",
    "                        w_vec = np.array([max(i, 0) for i in w_vec])\n",
    "                    w_vec = w_vec/np.sum(w_vec)\n",
    "                    \n",
    "                    if uniqueiters > 20:\n",
    "                        #quit trying -- speed is important\n",
    "                        SummarySet = np.random.choice(range(N_ground), K_budget, replace=False)\n",
    "                        uniquesummflag = 1\n",
    "                        print(\"switched to random selection for this round\")\n",
    "                \n",
    "                #if it is indeed unique\n",
    "                #the loop will automatically be exited\n",
    "                        \n",
    "            #here, we now have a unique SummarySet\n",
    "            #proceed as before\n",
    "            \n",
    "            Uncertainty_Queried_Summaries[jdx, :] = SummarySet\n",
    "            UQSummaries_dict[frozenset(SummarySet)] = jdx\n",
    "\n",
    "            # GET THE SCORE (for this jdx)\n",
    "            y = funcs.score_image_summarization(subset, np.transpose(Data_Mat_Complete), SummarySet)[0]\n",
    "            y = (y-ymin)/(ymax-ymin) #this CAN be negative\n",
    "            unquantized_score_vec[jdx] = y\n",
    "            \n",
    "#            # ADD NOISE TO FEEDBACK\n",
    "#            #amp_signal ~ 0.05\n",
    "#            amp_signal = 0.03\n",
    "#            amp_noise = amp_signal/math.pow(10, eta/20)\n",
    "#            y += np.random.normal(noise_offset, amp_noise)\n",
    "                        \n",
    "#            if N_bins != 0: #discretized feedback\n",
    "#                ydisc = 0\n",
    "#                for t in range(L-1):\n",
    "#                    ydisc += float(y >= thresh_learn[t])\n",
    "#                y = ydisc\n",
    "            \n",
    "            if L != 0: #discretized feedback\n",
    "                y = np.floor(L*y)\n",
    "                \n",
    "            Uncertainty_Queried_Scores[jdx] = y\n",
    "            \n",
    "            if alpha_ind == 6:\n",
    "                if alpha != None: #still increasing\n",
    "                    if alpha > 100000: #if large\n",
    "                        alpha = None #set to None to avoid reaching inf\n",
    "                    elif y > 0.5: #not yet large\n",
    "                        alpha = 10*alpha #make larger\n",
    "            \n",
    "            # UPDATE AND LEARN\n",
    "            #training matrix\n",
    "            x = funcs.featurize_data(Data_Mat, feature_based_func_type, gamma, SummarySet)\n",
    "            x = np.atleast_2d(x) ## 1 x N_dim\n",
    "\n",
    "            #learn\n",
    "            Cinv += float(1)/pow(noise_level,2) * np.dot(np.transpose(x), x) ## N_dim x N_dim\n",
    "            Yvec += np.transpose(float(1)/pow(noise_level,2) * np.dot(y, x)) ## N_dim x 1\n",
    "            w_vec = np.squeeze(np.dot(np.linalg.inv(Cinv), Yvec)) ## N_dim\n",
    "            \n",
    "            if threshold_zero_flag == 1:\n",
    "                w_vec = np.array([max(i, 0) for i in w_vec])\n",
    "                \n",
    "            #print('Sum of w_vec : ', np.sum(w_vec))\n",
    "            w_vec = w_vec/np.sum(w_vec)\n",
    "    \n",
    "            \n",
    "            # EVALUATE\n",
    "            #run greedy\n",
    "            optimized_summary = GreedySubmodKnapsackWeighted.GreedySubmodKnapsackWeighted(np.transpose(Data_Mat), feature_based_func_type_feedback, w_vec, V, K_budget, [1 for i in range(N_ground)], 1, gamma_feedback)\n",
    "            optimized_y = funcs.score_image_summarization(subset, np.transpose(Data_Mat_Complete), optimized_summary)[0]\n",
    "            \n",
    "            optimized_y = optimized_y/ymax #prevented from being negative\n",
    "\n",
    "            perf_vec[jdx] = optimized_y\n",
    "            coeff_vec[jdx] = w_vec\n",
    "            \n",
    "        # SAVING\n",
    "\n",
    "        # all are of length N_samples\n",
    "        savedict['Performance'] += perf_vec/idxrange\n",
    "        savedict['Learned_Vec'][idx] = coeff_vec\n",
    "        savedict['Thresh'][idx] = thresh_learn_vec\n",
    "        savedict['Queried_Summaries'][idx] = Uncertainty_Queried_Summaries\n",
    "        savedict['Queried_Scores'][idx] = Uncertainty_Queried_Scores\n",
    "        savedict['Unquant_Scores'][idx] = unquantized_score_vec\n",
    "\n",
    "    opfile = open(data_dir + outfile + '.save', 'wb')\n",
    "    cPickle.dump(savedict, opfile)\n",
    "    opfile.close()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "# RUN!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "ndimlist = np.array([5,10,20,50,100,150,200,300,500,0])\n",
    "ndimlist = np.array([100])\n",
    "L = 5\n",
    "\n",
    "for n_dim in ndimlist:\n",
    "    create_results(n_dim, 'dsopt', '0', L)\n",
    "    create_results(n_dim, 'greedy', '0', L)\n",
    "    create_results(n_dim, 'random', '0', L)\n",
    "    create_results(n_dim, 'random', '1', L)\n",
    "    create_results(n_dim, 'random', '2', L)\n",
    "    create_results(n_dim, 'random', '3', L)\n",
    "    create_results(n_dim, 'random', '4', L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
